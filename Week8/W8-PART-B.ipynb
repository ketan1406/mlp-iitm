{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNeuPz2qvA7lsZt2D2f6aA5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2GS99PE4cCsP","executionInfo":{"status":"ok","timestamp":1741532752965,"user_tz":-330,"elapsed":1988,"user":{"displayName":"24DS2000071 KETAN SAINI","userId":"09541276271295601432"}},"outputId":"8d0252f4-a9c4-4f51-a5b9-542d051eaef9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best cross-validated score:  0.9807692307692308\n"]}],"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn import svm\n","\n","def compute_GridSearchCV(kernels, regularizations, gamma='auto', cv=4, random_state=0):\n","    # Load the Iris dataset\n","    iris = load_iris()\n","    X, y = iris.data, iris.target\n","\n","    # Split the dataset into 70% train and 30% test\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n","\n","    # Define the parameter grid\n","    param_grid = {\n","        'kernel': kernels,\n","        'C': regularizations,\n","        'gamma': [gamma]\n","    }\n","\n","    # Initialize the model (SVC) with given random state\n","    model = svm.SVC(random_state=random_state)\n","\n","    # Initialize GridSearchCV with cross-validation = 4\n","    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv)\n","    grid.fit(X_train, y_train)\n","\n","    # Return the mean cross-validated score of the best estimator\n","    return grid.best_score_\n","\n","# Hyperparameters as specified\n","kernels = ['linear', 'rbf']\n","regularizations = [1, 15, 25]\n","\n","# Compute the mean cross-validated score using the function\n","best_score = compute_GridSearchCV(kernels, regularizations, gamma='auto', cv=4, random_state=0)\n","print(\"Best cross-validated score: \", best_score)\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8ZLEzoFcycs","executionInfo":{"status":"ok","timestamp":1741532990791,"user_tz":-330,"elapsed":34947,"user":{"displayName":"24DS2000071 KETAN SAINI","userId":"09541276271295601432"}},"outputId":"0cf00aaf-2cfd-430f-911d-2959dcac0b05"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","# Read the dataset.\n","# The Social_Network_Ads dataset is assumed to have a header row.\n","df = pd.read_csv(\"/content/drive/MyDrive/MLP/GA/Week8/Social_Network_Ads.csv\")\n","print(\"Dataset head:\")\n","print(df.head())\n","\n","# For this assignment we use \"Age\" and \"EstimatedSalary\" as features\n","# and \"Purchased\" as the target.\n","X = df[['Age', 'EstimatedSalary']]\n","y = df['Purchased']\n","\n","# Split the dataset into training and test sets (75:25 ratio)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n","\n","# Fit and transform the training features and transform the test features using StandardScaler.\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Fit a linear SVM (SVC with linear kernel and random_state=0) on the training data.\n","model = SVC(kernel='linear', random_state=0)\n","model.fit(X_train_scaled, y_train)\n","\n","# Predict on the test set\n","y_pred = model.predict(X_test_scaled)\n","\n","# Calculate the accuracy score\n","acc = accuracy_score(y_test, y_pred)\n","print(\"\\nAccuracy score on test set:\", acc)\n","\n","# Calculate the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"\\nConfusion Matrix:\")\n","print(cm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oO1INQbgcMRZ","executionInfo":{"status":"ok","timestamp":1741533006117,"user_tz":-330,"elapsed":1205,"user":{"displayName":"24DS2000071 KETAN SAINI","userId":"09541276271295601432"}},"outputId":"67431f3b-2907-40ef-e109-96e27a0621e0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset head:\n","   Age  EstimatedSalary  Purchased\n","0   19            19000          0\n","1   35            20000          0\n","2   26            43000          0\n","3   27            57000          0\n","4   19            76000          0\n","\n","Accuracy score on test set: 0.9\n","\n","Confusion Matrix:\n","[[66  2]\n"," [ 8 24]]\n"]}]},{"cell_type":"code","source":["# Part 1: Linear SVC with MinMaxScaler on MNIST (first 20,000 for training, next 5,000 for testing)\n","import numpy as np\n","from sklearn.datasets import fetch_openml\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n","\n","# Fetch MNIST (this may take a moment)\n","mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n","X, y = mnist[\"data\"], mnist[\"target\"]\n","\n","# Convert targets to integers\n","y = y.astype(np.int8)\n","\n","# Select first 20,000 samples as training and next 5,000 as test\n","X_train_part = X[:20000]\n","y_train_part = y[:20000]\n","X_test_part = X[20000:25000]\n","y_test_part = y[20000:25000]\n","\n","# Build a pipeline with MinMaxScaler and SVC with linear kernel, decision_function_shape='ovr'\n","pipe_linear = make_pipeline(MinMaxScaler(),\n","                            SVC(kernel='linear', decision_function_shape='ovr', class_weight=None, random_state=0))\n","\n","# Fit the pipeline on the training data and predict on test data\n","pipe_linear.fit(X_train_part, y_train_part)\n","y_pred_linear = pipe_linear.predict(X_test_part)\n","\n","# Compute the confusion matrix and sum of diagonal elements\n","cm = confusion_matrix(y_test_part, y_pred_linear)\n","diag_sum = np.trace(cm)\n","print(\"Confusion Matrix for Linear SVC:\")\n","print(cm)\n","print(\"Sum of main diagonal elements:\", diag_sum)\n","\n","# Compute overall precision, recall, and f1-score (macro averaging gives an overall idea,\n","# but here we show the weighted metrics as well for consistency)\n","prec = precision_score(y_test_part, y_pred_linear, average='weighted')\n","rec = recall_score(y_test_part, y_pred_linear, average='weighted')\n","f1 = f1_score(y_test_part, y_pred_linear, average='weighted')\n","print(\"\\nEvaluation metrics for Linear SVC on MNIST:\")\n","print(f\"Precision: {prec:.2f}, Recall: {rec:.2f}, F1 Score: {f1:.2f}\")\n","\n","# Expected answers (closest options):\n","# Q4 (sum of diagonal): 4623\n","# Q5 (Precision/Recall/F1): 0.92, 0.92, 0.92\n","\n","###############################################################################\n","# Part 2: Poly-kernel SVM with StandardScaler on MNIST (50:50 train-test split)\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","# Split the MNIST dataset into 50:50 ratio\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n","\n","# Build a pipeline with StandardScaler and SVC with poly kernel and specified parameters\n","pipe_poly = make_pipeline(StandardScaler(),\n","                          SVC(kernel='poly', degree=3, decision_function_shape='ovr',\n","                              class_weight='balanced', C=10, random_state=0))\n","\n","# Fit the pipeline and predict on test data\n","pipe_poly.fit(X_train, y_train)\n","y_pred_poly = pipe_poly.predict(X_test)\n","\n","# Generate the classification report\n","report = classification_report(y_test, y_pred_poly, output_dict=True)\n","weighted_f1 = report['weighted avg']['f1-score']\n","print(\"\\nClassification Report for Poly-kernel SVC:\")\n","print(classification_report(y_test, y_pred_poly))\n","print(\"Weighted Average F1 Score:\", weighted_f1)\n","\n","# Expected answer for weighted avg f1-score: around 0.97"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f3uwEs_qdJwf","outputId":"a79355fc-179b-4e83-c055-0d4072b4b21f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix for Linear SVC:\n","[[469   0   1   0   0   5   1   1   1   0]\n"," [  1 555   2   4   0   1   1   1   3   0]\n"," [  5   4 479   6   9   1   3   6   7   1]\n"," [  3   3  13 462   0  19   0   3   9   4]\n"," [  2   0   6   0 468   1   2   4   1  16]\n"," [ 14   0   0  18   2 405   3   2   7   9]\n"," [  1   2   2   0   6   9 471   0   0   0]\n"," [  0   2   4   1   7   1   0 483   0   6]\n"," [  3  10  11  15   0  19   6   3 396   3]\n"," [  3   6   1   2  21   0   0  26   2 435]]\n","Sum of main diagonal elements: 4623\n","\n","Evaluation metrics for Linear SVC on MNIST:\n","Precision: 0.92, Recall: 0.92, F1 Score: 0.92\n"]}]},{"cell_type":"code","source":["# Import necessary modules\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","# Function 1: Using poly kernel, C=10, gamma='auto'\n","def compute_score_poly(X_train, y_train, X_test, y_test):\n","    # Create the model with poly kernel, regularization parameter 10, gamma 'auto'\n","    model = SVC(kernel='poly', C=10, gamma='auto', random_state=42)\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","# Function 2: Using sigmoid kernel, C=25, gamma='auto'\n","def compute_score_sigmoid(X_train, y_train, X_test, y_test):\n","    # Create the model with sigmoid kernel, regularization parameter 25, gamma 'auto'\n","    model = SVC(kernel='sigmoid', C=25, gamma='auto', random_state=42)\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    return accuracy_score(y_test, y_pred)\n","\n","# Function 3: Build a pipeline (Scaler and SVC), dropping Iris-setosa rows and computing precision\n","def compute_precision_score():\n","    # Load the Iris dataset\n","    iris = load_iris()\n","    # Create a DataFrame for easier manipulation\n","    import pandas as pd\n","    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n","    df['target'] = iris.target\n","    # In Iris: 0 = Iris-setosa, 1 = Iris-versicolor, 2 = Iris-virginica.\n","    # Drop rows where target is 0 (Iris-setosa)\n","    df_filtered = df[df['target'] != 0]\n","\n","    # Features and target for the filtered dataset\n","    X = df_filtered[iris.feature_names]\n","    y = df_filtered['target']\n","\n","    # Create a pipeline with MinMaxScaler (named Scaler) and SVC (default parameters)\n","    pipeline = make_pipeline(MinMaxScaler(), SVC(random_state=0))\n","\n","    # Split data into 75:25 ratio with random_state=0\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n","\n","    # Fit the pipeline and predict\n","    pipeline.fit(X_train, y_train)\n","    y_pred = pipeline.predict(X_test)\n","\n","    # Compute the weighted precision score\n","    return precision_score(y_test, y_pred, average='weighted')\n","\n","# Main block: Split the Iris dataset for the first two functions\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data: 70% training, 30% test, random_state=42\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Compute and print scores for each part\n","accuracy_poly = compute_score_poly(X_train, y_train, X_test, y_test)\n","accuracy_sigmoid = compute_score_sigmoid(X_train, y_train, X_test, y_test)\n","precision_val = compute_precision_score()\n","\n","print(\"Accuracy score using poly kernel (C=10, gamma='auto'):\", accuracy_poly)\n","print(\"Accuracy score using sigmoid kernel (C=25, gamma='auto'):\", accuracy_sigmoid)\n","print(\"Weighted Precision score (after dropping Iris-setosa):\", precision_val)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sefEZ5I5dxen","executionInfo":{"status":"ok","timestamp":1741533529724,"user_tz":-330,"elapsed":1602,"user":{"displayName":"24DS2000071 KETAN SAINI","userId":"09541276271295601432"}},"outputId":"c90e34d0-62f9-4e0b-baae-b6b0043b648d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy score using poly kernel (C=10, gamma='auto'): 1.0\n","Accuracy score using sigmoid kernel (C=25, gamma='auto'): 0.28888888888888886\n","Weighted Precision score (after dropping Iris-setosa): 0.9314285714285714\n"]}]}]}